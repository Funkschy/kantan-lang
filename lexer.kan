import "std";
import "span";
import "path";
import "source";

extern def tok2str(ty: TokenType): string;

// Token

type TokenType enum {
    UnknownErr,
    CharErr,
    EscapeErr,

    Int,
    Float,
    Null,
    Ident,

    Let,
    If,
    Else,
    Import,
    Def,
    While,
    New,
    Delete,
    Return,
    Type,
    Struct,
    Enum,
    As,
    Extern,
    Sizeof,
    For,
    Undefined,

    Semi,
    Comma,
    Colon,
    Dot,
    TripleDot,
    Bang,
    BangEq,
    DoubleEq,

    Plus,
    Minus,
    Star,
    Slash,
    Percent,

    Eq,
    PlusEq,
    MinusEq,
    StarEq,
    SlashEq,
    PercentEq,

    Smaller,
    SmallerEq,
    Greater,
    GreaterEq,

    Ampersand,
    DoubleAmpersand,
    Pipe,
    DoublePipe,

    LParen,
    RParen,
    LBrace,
    RBrace,
    String,
    Char,

    EOF
}

type Token struct {
    ty: TokenType,
    span: span.Span
}

def empty_token(): Token {
    return Token { ty: TokenType.UnknownErr, span: span.empty() };
}

def lexeme(tok: *Token): string {
    return tok.span.start;
}

def is_err(token: *Token): bool {
    return token.ty == TokenType.UnknownErr
        || token.ty == TokenType.CharErr
        || token.ty == TokenType.EscapeErr;
}

// static (don't free!)
def token_ty_to_static_string(ty: TokenType): string {
    return tok2str(ty);
}

def token_len(token: *Token): i32 {
    return span.len(token.span);
}

// returned boolean indicates if the strings has to be freed
def token_to_string(token: *Token, dest: *string): bool {
    // this is the range of tokens, that need to be formatted
    if token.ty >= TokenType.Int && token.ty <= TokenType.Ident {
        let len = token_len(token);
        *dest = std.strndup(lexeme(token), len);
        return true;
    }

    if token.ty == TokenType.String || token.ty == TokenType.Char {
        let len = token_len(token);
        *dest = std.strndup(lexeme(token) - 1, len + 2);
        return true;
    }

    *dest = tok2str(token.ty);
    return false;
}

def is_assign(ty: TokenType): bool {
    return ty >= TokenType.Eq && ty <= TokenType.PercentEq;
}

// Lexer

type Lexer struct {
    has_peek: bool,
    peek: Token,
    source: *source.SourceFile,
    start: string,
    current: string
}

def create(src: *source.SourceFile): Lexer {
    let span = span.create(src.code, src.code);
    let init_peek = Token { ty: TokenType.UnknownErr, span: span };

    return Lexer {
        has_peek: false,
        peek: init_peek,
        source: src,
        start: src.code,
        current: src.code
    };
}

def end_pos(l: *Lexer): i32 {
    return l.source.len - 1;
}

def peek(l: *Lexer): Token {
    if l.has_peek {
        return l.peek;
    }

    l.peek = next_token(l);
    l.has_peek = true;
    return l.peek;
}

def next_token(l: *Lexer): Token {
    if l.has_peek {
        l.has_peek = false;
        return l.peek;
    }

    skip_whitespace(l);
    return get_next_token(l);
}

def current_pos(l: *Lexer): i32 {
    if l.has_peek {
        return l.peek.span.start - l.source.code;
    }

    return l.current - l.source.code;
}

def current_ptr(l: *Lexer): string {
    if l.has_peek {
        return l.peek.span.start;
    }

    return l.current;
}

def current(l: *Lexer): i32 {
    let c = 0;
    let read_bytes = current_pos(l);
    std.read_char(l.current, l.source.len - read_bytes, &c);
    return c;
}

def source_file(l: *Lexer): *source.SourceFile {
    return l.source;
}

def is_num(c: i32): bool {
    return c >= std.char_to_int('0') && c <= std.char_to_int('9');
}

def is_letter(c: i32): bool {
    return (c >= std.char_to_int('a') && c <= std.char_to_int('z'))
        || (c >= std.char_to_int('A') && c <= std.char_to_int('Z'));
}

def check_keyword(l: *Lexer, start: i32, rest_len: i32, rest: string, ty: TokenType): TokenType {
    if l.current - l.start == start + rest_len {
        if std.memcmp((l.start + start) as *void, rest as *void, rest_len) == 0 {
            return ty;
        }
    }

    return TokenType.Ident;
}

def ident_type(l: *Lexer): TokenType {
    let bytes_until = l.start - l.source.code;
    let start = 0;
    let len = std.read_char(l.start, l.source.len - bytes_until, &start);

    if start == std.char_to_int('l') {
        return check_keyword(l, 1, 2, "et", TokenType.Let);
    } else if start == std.char_to_int('i') && l.current - l.start > 1 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == std.char_to_int('f') {
            return check_keyword(l, 2, 0, "", TokenType.If);
        } else if next == std.char_to_int('m') {
            return check_keyword(l, 2, 4, "port", TokenType.Import);
        }
    } else if start == std.char_to_int('d') && l.current - l.start > 2 {
        let next = 0;
        len = len + std.read_char(l.start + len, l.source.len - bytes_until - len, &next);
        // read third char
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == std.char_to_int('f') {
            return check_keyword(l, 1, 2, "ef", TokenType.Def);
        } else if next == std.char_to_int('l') {
            return check_keyword(l, 1, 5, "elete", TokenType.Delete);
        }
    } else if start == std.char_to_int('e') && l.current - l.start > 3 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == std.char_to_int('x') {
            return check_keyword(l, 2, 4, "tern", TokenType.Extern);
        } else if next == std.char_to_int('l') {
            return check_keyword(l, 2, 2, "se", TokenType.Else);
        } else if next == std.char_to_int('n') {
            return check_keyword(l, 2, 2, "um", TokenType.Enum);
        }
    } else if start == std.char_to_int('n') && l.current - l.start > 2 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == std.char_to_int('u') {
            return check_keyword(l, 2, 2, "ll", TokenType.Null);
        } else if next == std.char_to_int('e') {
            return check_keyword(l, 2, 1, "w", TokenType.New);
        }
    } else if start == std.char_to_int('s') && l.current - l.start > 5 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == std.char_to_int('t') {
            return check_keyword(l, 2, 4, "ruct", TokenType.Struct);
        } else if next == std.char_to_int('i') {
            return check_keyword(l, 2, 4, "zeof", TokenType.Sizeof);
        }
    } else if start == std.char_to_int('w') {
        return check_keyword(l, 1, 4, "hile", TokenType.While);
    } else if start == std.char_to_int('t') {
        return check_keyword(l, 1, 3, "ype", TokenType.Type);
    } else if start == std.char_to_int('r') {
        return check_keyword(l, 1, 5, "eturn", TokenType.Return);
    } else if start == std.char_to_int('a') {
        return check_keyword(l, 1, 1, "s", TokenType.As);
    } else if start == std.char_to_int('f') {
        return check_keyword(l, 1, 2, "or", TokenType.For);
    } else if start == std.char_to_int('u') {
        return check_keyword(l, 1, 8, "ndefined", TokenType.Undefined);
    }

    return TokenType.Ident;
}

def lex_ident(l: *Lexer): Token {
    while is_letter(current(l)) || is_num(current(l)) || current(l) == std.char_to_int('_') {
        advance(l);
    }

    return token_from_start(l, ident_type(l));
}

def lex_num(l: *Lexer): Token {
    while is_num(current(l)) {
        advance(l);
    }

    // float
    if current(l) == std.char_to_int('.') {
        advance(l);
        while is_num(current(l)) {
            advance(l);
        }

        return token_from_start(l, TokenType.Float);
    }

    return token_from_start(l, TokenType.Int);
}

def lex_char_sequence(l: *Lexer, terminator: i32, ty: TokenType): Token {
    let backslash = 92;

    let error_start: string = null;
    let error_end: string = null;

    let continue = true;
    let escape_next = false;
    while !at_end(l) && continue {
        let curr = current(l);

        if curr == backslash && !escape_next {
            escape_next = true;
        } else if curr == terminator {
            continue = escape_next;
            escape_next = false;
        } else {
            if escape_next {
                // every escapable character
                if curr != std.char_to_int('0')
                    && curr != std.char_to_int('n')
                    && curr != std.char_to_int('r')
                    && curr != std.char_to_int('t')
                    && curr != std.char_to_int('\\')
                    && curr != 39 // single quote
                    && curr != std.char_to_int('"') {

                    error_start = l.current - 1;
                    error_end = l.current + 1;
                }
            }

            escape_next = false;
        }

        advance(l);
    }

    if error_start != null {
        return create_token(l, TokenType.EscapeErr, error_start, error_end);
    }

    return create_token(l, ty, l.start + 1, l.current - 1);
}

def lex_string(l: *Lexer): Token {
    return lex_char_sequence(l, std.char_to_int('"'), TokenType.String);
}

def lex_char(l: *Lexer): Token {
    let char_seq = lex_char_sequence(l, std.char_to_int(*"'"), TokenType.Char);

    if is_err(&char_seq) {
        return char_seq;
    }

    let len = l.current - l.start;
    if len == 4 && *(l.start + 1) == '\\' {
        return create_token(l, TokenType.Char, l.start + 1, l.current - 1);
    }

    if len != 3 {
        return create_token(l, TokenType.CharErr, l.start + 1, l.current - 1);
    }

    return create_token(l, TokenType.Char, l.start + 1, l.current - 1);
}

def is_whitespace(c: i32): bool {
    return c == std.char_to_int(' ')
        || c == std.char_to_int('\r')
        || c == std.char_to_int('\n')
        || c == std.char_to_int('\t')
        || c == std.char_to_int('\0');
}

def skip_until(l: *Lexer, ch: i32): void {
    while !at_end(l) {
        advance(l);
        if current(l) == ch {
            return;
        }
    }
}

def skip_whitespace(l: *Lexer): void {
    let continue = true;
    while !at_end(l) && continue {
        let c = current(l);
        continue = is_whitespace(c);
        if continue {
            advance(l);
        }
    }
}

def at_end(l: *Lexer): bool {
    return *l.current == '\0';
}

def advance(l: *Lexer): i32 {
    let read_bytes = current_pos(l);
    let c = 0;
    let len = std.read_char(l.current, l.source.len - read_bytes, &c);

    l.current = l.current + len;
    return c;
}

def create_token(l: *Lexer, ty: TokenType, start: string, end: string): Token {
    return Token {
        ty: ty,
        span: span.create(start, end)
    };
}

def create_token_from_span(l: *Lexer, ty: TokenType, start: string, span: span.Span): Token {
    return Token {
        ty: ty,
        span: span
    };
}

def token_from_start(l: *Lexer, ty: TokenType): Token {
    return create_token(l, ty, l.start, l.current);
}

def get_next_token(l: *Lexer): Token {
    if at_end(l) {
        let ptr = current_ptr(l);
        return Token {
            ty: TokenType.EOF,
            span: span.create(ptr, ptr)
        };
    }

    l.start = l.current;
    let c = advance(l);

    if is_num(c) {
        return lex_num(l);
    }

    if is_letter(c) {
        return lex_ident(l);
    }

    if c == std.char_to_int('"') {
        return lex_string(l);
    }

    if c == std.char_to_int(*"'") {
        return lex_char(l);
    }

    if c == std.char_to_int(';') {
        return token_from_start(l, TokenType.Semi);
    }

    if c == std.char_to_int(',') {
        return token_from_start(l, TokenType.Comma);
    }

    if c == std.char_to_int(':') {
        return token_from_start(l, TokenType.Colon);
    }

    if c == std.char_to_int('+') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.PlusEq);
        }
        return token_from_start(l, TokenType.Plus);
    }

    if c == std.char_to_int('-') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.MinusEq);
        }
        return token_from_start(l, TokenType.Minus);
    }

    if c == std.char_to_int('*') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.StarEq);
        }
        return token_from_start(l, TokenType.Star);
    }

    if c == std.char_to_int('(') {
        return token_from_start(l, TokenType.LParen);
    }

    if c == std.char_to_int(')') {
        return token_from_start(l, TokenType.RParen);
    }

    if c == std.char_to_int('{') {
        return token_from_start(l, TokenType.LBrace);
    }

    if c == std.char_to_int('}') {
        return token_from_start(l, TokenType.RBrace);
    }

    if c == std.char_to_int('%') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.PercentEq);
        }
        return token_from_start(l, TokenType.Percent);
    }

    if c == std.char_to_int('.') {
        if std.memcmp(l.start as *void, "..." as *void, 3) == 0 {
            advance(l);
            advance(l);
            return token_from_start(l, TokenType.TripleDot);
        }

        return token_from_start(l, TokenType.Dot);
    }

    if c == std.char_to_int('/') {
        if current(l) == std.char_to_int('/') {
            skip_until(l, std.char_to_int('\n'));
            return next_token(l);
        }
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.SlashEq);
        }
        return token_from_start(l, TokenType.Slash);
    }

    if c == std.char_to_int('<') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.SmallerEq);
        }
        return token_from_start(l, TokenType.Smaller);
    }

    if c == std.char_to_int('>') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.GreaterEq);
        }
        return token_from_start(l, TokenType.Greater);
    }

    if c == std.char_to_int('&') {
        if current(l) == std.char_to_int('&') {
            advance(l);
            return token_from_start(l, TokenType.DoubleAmpersand);
        }
        return token_from_start(l, TokenType.Ampersand);
    }

    if c == std.char_to_int('|') {
        if current(l) == std.char_to_int('|') {
            advance(l);
            return token_from_start(l, TokenType.DoublePipe);
        }
        return token_from_start(l, TokenType.Pipe);
    }

    if c == std.char_to_int('=') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.DoubleEq);
        }
        return token_from_start(l, TokenType.Eq);
    }

    if c == std.char_to_int('!') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TokenType.BangEq);
        }
        return token_from_start(l, TokenType.Bang);
    }


    return token_from_start(l, TokenType.UnknownErr);
}

