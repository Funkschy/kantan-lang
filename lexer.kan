#include "token_types.h"
#include "std.h"

import "std";
import "span";
// TODO: only debug
import "io";

extern def tok2str(ty: i32): string;

// Token

type Token struct {
    ty: i32,
    span: span.Span,
    lexeme: string
}

def empty_token(): Token {
    return Token { ty: TOKEN_ERR, span: span.empty(), lexeme: null };
}

// static (don't free!)
def token_ty_to_static_string(ty: i32): string {
    return tok2str(ty);
}

def token_len(token: *Token, si: *span.SpanInterner): i32 {
    return span.get(si, token.span).len;
}

// returned boolean indicates if the strings has to be freed
def token_to_string(token: *Token, si: *span.SpanInterner, dest: *string): bool {
    // this is the range of tokens, that need to be formatted
    if token.ty >= TOKEN_INT && token.ty <= TOKEN_IDENT {
        let len = token_len(token, si);
        *dest = std.strndup(token.lexeme, len);
        return true;
    }

    *dest = tok2str(token.ty);
    return false;
}

// Lexer

type Lexer struct {
    has_peek: bool,
    ctx: i32,
    source_len: i32,
    peek: Token,
    source: string,
    start: string,
    current: string,
    span_interner: *span.SpanInterner
}

def create(source: string, span_interner: *span.SpanInterner, ctx: i32): Lexer {
    let span = span.create(span_interner, 0, 0, ctx);
    let init_peek = Token { ty: TOKEN_ERR, span: span, lexeme: null };

    return Lexer {
        has_peek: false,
        ctx: ctx,
        source_len: std.strlen(source), // TODO: save length while reading file
        peek: init_peek,
        source: source,
        start: source,
        current: source,
        span_interner: span_interner
    };
}

def end_pos(l: *Lexer): i32 {
    return l.source_len - 1;
}

def peek(l: *Lexer): Token {
    if l.has_peek {
        return l.peek;
    }

    l.peek = next_token(l);
    l.has_peek = true;
    return l.peek;
}

def next_token(l: *Lexer): Token {
    if l.has_peek {
        l.has_peek = false;
        return l.peek;
    }

    skip_whitespace(l);
    return get_next_token(l);
}

def current_pos(l: *Lexer): i32 {
    if l.has_peek {
        return l.peek.span.start;
    }

    return l.current - l.source;
}

def current(l: *Lexer): i32 {
    let c = 0;
    let read_bytes = current_pos(l);
    std.read_char(l.current, l.source_len - read_bytes, &c);
    return c;
}

def is_num(c: i32): bool {
    return c >= std.char_to_int('0') && c <= std.char_to_int('9');
}

def parse_num(l: *Lexer): Token {
    while is_num(current(l)) {
        advance(l);
    }

    return token_from_start(l, TOKEN_INT);
}

def parse_string(l: *Lexer): Token {
    while !at_end(l) && current(l) != std.char_to_int('"') {
        advance(l);
    }
    advance(l);

    return create_token(l, TOKEN_STRING, l.start + 1, l.current - 1);
}

def is_letter(c: i32): bool {
    return (c >= std.char_to_int('a') && c <= std.char_to_int('z'))
        || (c >= std.char_to_int('A') && c <= std.char_to_int('Z'));
}

def check_keyword(l: *Lexer, start: i32, rest_len: i32, rest: string, ty: i32): i32 {
    if l.current - l.start == start + rest_len {
        if std.memcmp((l.start + start) as *void, rest as *void, rest_len) == 0 {
            return ty;
        }
    }

    return TOKEN_IDENT;
}

def ident_type(l: *Lexer): i32 {
    let bytes_until = l.start - l.source;
    let start = 0;
    let len = std.read_char(l.start, l.source_len - bytes_until, &start);

    if start == std.char_to_int('l') {
        return check_keyword(l, 1, 2, "et", TOKEN_LET);
    } else if start == std.char_to_int('i') && l.current - l.start > 1 {
        let next = 0;
        std.read_char(l.start + len, l.source_len - bytes_until - len, &next);

        if next == std.char_to_int('f') {
            return TOKEN_IF;
        } else if next == std.char_to_int('m') {
            return check_keyword(l, 2, 4, "port", TOKEN_IMPORT);
        }
    } else if start == std.char_to_int('d') && l.current - l.start > 2 {
        let next = 0;
        len = len + std.read_char(l.start + len, l.source_len - bytes_until - len, &next);
        // read third char
        std.read_char(l.start + len, l.source_len - bytes_until - len, &next);

        if next == std.char_to_int('f') {
            return check_keyword(l, 1, 2, "ef", TOKEN_DEF);
        } else if next == std.char_to_int('l') {
            return check_keyword(l, 1, 5, "elete", TOKEN_DELETE);
        }
    } else if start == std.char_to_int('e') {
        return check_keyword(l, 1, 3, "lse", TOKEN_ELSE);
    } else if start == std.char_to_int('w') {
        return check_keyword(l, 1, 4, "hile", TOKEN_WHILE);
    } else if start == std.char_to_int('n') {
        return check_keyword(l, 1, 2, "ew", TOKEN_NEW);
    } else if start == std.char_to_int('s') {
        return check_keyword(l, 1, 5, "truct", TOKEN_STRUCT);
    } else if start == std.char_to_int('t') {
        return check_keyword(l, 1, 3, "ype", TOKEN_TYPE);
    } else if start == std.char_to_int('r') {
        return check_keyword(l, 1, 5, "eturn", TOKEN_RETURN);
    }

    return TOKEN_IDENT;
}

def parse_ident(l: *Lexer): Token {
    while is_letter(current(l)) || is_num(current(l)) || current(l) == std.char_to_int('_') {
        advance(l);
    }

    return token_from_start(l, ident_type(l));
}

def is_whitespace(c: i32): bool {
    return c == std.char_to_int(' ')
        || c == std.char_to_int('\r')
        || c == std.char_to_int('\n')
        || c == std.char_to_int('\t')
        || c == std.char_to_int('\0');
}

def skip_until(l: *Lexer, ch: i32): void {
    while !at_end(l) {
        advance(l);
        if current(l) == ch {
            return;
        }
    }
}

def skip_whitespace(l: *Lexer): void {
    let continue = true;
    while !at_end(l) && continue {
        let c = current(l);
        continue = is_whitespace(c);
        if continue {
            advance(l);
        }
    }
}

def at_end(l: *Lexer): bool {
    return *l.current == '\0';
}

def advance(l: *Lexer): i32 {
    let read_bytes = current_pos(l);
    let c = 0;
    let len = std.read_char(l.current, l.source_len - read_bytes, &c);

    l.current = l.current + len;
    return c;
}

def create_span(l: *Lexer, start: i32, end: i32): span.Span {
    return span.create(l.span_interner, start, end, l.ctx);
}

def create_token(l: *Lexer, ty: i32, start: string, end: string): Token {
    return Token {
        ty: ty,
        span: create_span(l, start - l.source, end - l.source),
        lexeme: start
    };
}

def create_token_from_span(l: *Lexer, ty: i32, start: string, span: span.Span): Token {
    return Token {
        ty: ty,
        span: span,
        lexeme: start
    };
}

def token_from_start(l: *Lexer, ty: i32): Token {
    return create_token(l, ty, l.start, l.current);
}

def get_next_token(l: *Lexer): Token {
    if at_end(l) {
        let pos = current_pos(l);
        return Token {
            ty: TOKEN_EOF,
            span: create_span(l, pos, pos),
            lexeme: null
        };
    }

    l.start = l.current;
    let c = advance(l);

    if is_num(c) {
        return parse_num(l);
    }

    if is_letter(c) {
        return parse_ident(l);
    }

    if c == std.char_to_int('"') {
        return parse_string(l);
    }

    if c == std.char_to_int(';') {
        return token_from_start(l, TOKEN_SEMI);
    }

    if c == std.char_to_int(',') {
        return token_from_start(l, TOKEN_COMMA);
    }

    if c == std.char_to_int(':') {
        return token_from_start(l, TOKEN_COLON);
    }

    if c == std.char_to_int('.') {
        return token_from_start(l, TOKEN_DOT);
    }

    if c == std.char_to_int('+') {
        return token_from_start(l, TOKEN_PLUS);
    }

    if c == std.char_to_int('-') {
        return token_from_start(l, TOKEN_MINUS);
    }

    if c == std.char_to_int('*') {
        return token_from_start(l, TOKEN_STAR);
    }

    if c == std.char_to_int('(') {
        return token_from_start(l, TOKEN_LPAREN);
    }

    if c == std.char_to_int(')') {
        return token_from_start(l, TOKEN_RPAREN);
    }

    if c == std.char_to_int('{') {
        return token_from_start(l, TOKEN_LBRACE);
    }

    if c == std.char_to_int('}') {
        return token_from_start(l, TOKEN_RBRACE);
    }

    if c == std.char_to_int('/') {
        if current(l) == std.char_to_int('/') {
            skip_until(l, std.char_to_int('\n'));
            return next_token(l);
        }
        return token_from_start(l, TOKEN_SLASH);
    }

    if c == std.char_to_int('<') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TOKEN_SMALLER_EQ);
        }
        return token_from_start(l, TOKEN_SMALLER);
    }

    if c == std.char_to_int('>') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TOKEN_GREATER_EQ);
        }
        return token_from_start(l, TOKEN_GREATER);
    }

    if c == std.char_to_int('&') {
        if current(l) == std.char_to_int('&') {
            advance(l);
            return token_from_start(l, TOKEN_DOUBLE_AMPERSAND);
        }
        return token_from_start(l, TOKEN_AMPERSAND);
    }

    if c == std.char_to_int('|') {
        if current(l) == std.char_to_int('|') {
            advance(l);
            return token_from_start(l, TOKEN_DOUBLE_PIPE);
        }
        return token_from_start(l, TOKEN_PIPE);
    }

    if c == std.char_to_int('=') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TOKEN_DOUBLE_EQ);
        }
        return token_from_start(l, TOKEN_EQ);
    }

    if c == std.char_to_int('!') {
        if current(l) == std.char_to_int('=') {
            advance(l);
            return token_from_start(l, TOKEN_BANG_EQ);
        }
        return token_from_start(l, TOKEN_BANG);
    }


    return token_from_start(l, TOKEN_ERR);
}

