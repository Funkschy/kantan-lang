#include "token_types.h"
#include "std.h"

import "std"
// TODO: only debug
import "io"

extern def tok2str(ty: i32): string;

type Token struct {
    ty: i32,
    len: i32,
    lexeme: string
}

type Lexer struct {
    has_peek: bool,
    peek: Token,
    source: string,
    start: string,
    current: string
}

def create(source: string): Lexer {
    return Lexer {
        has_peek: false,
        peek: Token { ty: TOKEN_ERR, len: 0, lexeme: null },
        source: source,
        start: source,
        current: source
    };
}

def eof(): i32 {
    return TOKEN_EOF;
}

def peek(l: *Lexer): Token {
    if l.has_peek {
        return l.peek;
    }

    l.peek = next_token(l);
    l.has_peek = true;
    return l.peek;
}

def next_token(l: *Lexer): Token {
    if l.has_peek {
        l.has_peek = false;
        return l.peek;
    }

    skip_whitespace(l);
    return get_next_token(l);
}

def get_next_token(l: *Lexer): Token {
    if at_end(l) {
        return Token { ty: TOKEN_EOF, len: 0, lexeme: null };
    }

    l.start = l.current;
    let c = *l.current;
    advance(l);

    if is_num(c) {
        return parse_num(l);
    } else if is_letter(c) {
        return parse_ident(l);
    } else if c == ';' {
        return token_from_start(l, TOKEN_SEMI);
    } else if c == '=' {
        return token_from_start(l, TOKEN_EQ);
    } else if c == '+' {
        return token_from_start(l, TOKEN_PLUS);
    } else if c == '-' {
        return token_from_start(l, TOKEN_MINUS);
    } else if c == '*' {
        return token_from_start(l, TOKEN_STAR);
    } else if c == '/' {
        return token_from_start(l, TOKEN_SLASH);
    } else if c == '(' {
        return token_from_start(l, TOKEN_LPAREN);
    } else if c == ')' {
        return token_from_start(l, TOKEN_RPAREN);
    }

    return token_from_start(l, TOKEN_ERR);
}

def is_num(c: char): bool {
    return c >= '0' && c <= '9';
}

def parse_num(l: *Lexer): Token {
    while is_num(*l.current) {
        advance(l);
    }

    return token_from_start(l, TOKEN_INT);
}

def is_letter(c: char): bool {
    return (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z');
}

def check_keyword(l: *Lexer, start: i32, rest_len: i32, rest: string, ty: i32): i32 {
    if l.current - l.start == start + rest_len {
        if std.memcmp((l.start + start) as *void, rest as *void, rest_len) == 0 {
            return ty;
        }
    }

    return TOKEN_IDENT;
}

def ident_type(l: *Lexer): i32 {
    if *l.start == 'l' {
        return check_keyword(l, 1, 2, "et", TOKEN_LET);
    } else if *l.start == 'i' && l.current - l.start > 1 {
        if *(l.start + 1) == 'f' {
            return TOKEN_IF;
        } else if *(l.start + 1) == 'm' {
            return check_keyword(l, 2, 4, "port", TOKEN_IMPORT);
        }
    }

    return TOKEN_IDENT;
}

def parse_ident(l: *Lexer): Token {
    while is_letter(*l.current) {
        advance(l);
    }

    return token_from_start(l, ident_type(l));
}

def is_whitespace(c: char): bool {
    return c == ' ' || c == '\r' || c == '\n' || c == '\t' || c == '\0';
}

def skip_whitespace(l: *Lexer): void {
    let continue = true;
    while !at_end(l) && continue {
        let c = *l.current;
        continue = is_whitespace(c);
        if continue {
            advance(l);
        }
    }
}

def at_end(l: *Lexer): bool {
    return *l.current == '\0';
}

def advance(l: *Lexer): char {
    l.current = l.current + 1;
    return *(l.current - 1);
}

def token_from_start(l: *Lexer, ty: i32): Token {
    return Token { ty: ty, len: l.current - l.start, lexeme: l.start };
}

def token_to_string(token: *Token): string {
    return tok2str(token.ty);
}
