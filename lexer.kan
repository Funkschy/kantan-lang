import "std";
import "span";
import "source";

extern def tok2str(ty: i32): string;

// Token

type TokenType enum {
    UnknownErr,
    CharErr,
    EscapeErr,

    Int,
    Float,
    Null,
    Ident,

    Let,
    If,
    Else,
    Import,
    Def,
    While,
    New,
    Delete,
    Return,
    Type,
    Struct,
    Enum,
    Union,
    As,
    Extern,
    Sizeof,
    For,
    Undefined,
    Continue,
    Break,
    Defer,

    Semi,
    Comma,
    Colon,
    Dot,
    TripleDot,
    Bang,
    BangEq,
    DoubleEq,

    Plus,
    Minus,
    Star,
    Slash,
    Percent,

    Eq,
    PlusEq,
    MinusEq,
    StarEq,
    SlashEq,
    PercentEq,
    AmpersandEq,
    PipeEq,
    CaretEq,

    Smaller,
    SmallerEq,
    Greater,
    GreaterEq,

    Ampersand,
    DoubleAmpersand,
    Pipe,
    DoublePipe,
    Caret, // ^
    Tilde, // ~

    SmallerSmaller,
    GreaterGreater,

    LParen,
    RParen,
    LBrace,
    RBrace,
    String,
    Char,

    EOF
}

type Token struct {
    ty: TokenType,
    span: span.Span
}

def empty_token(): Token {
    return Token { ty: TokenType.UnknownErr, span: span.empty() };
}

def (tok: *Token) lexeme(): string {
    return tok.span.start;
}

def (tok: *Token) is_err(): bool {
    return tok.ty == TokenType.UnknownErr
        || tok.ty == TokenType.CharErr
        || tok.ty == TokenType.EscapeErr;
}

def (token: *Token) len(): usize {
    return token.span.len();
}

// static (don't free!)
def token_ty_to_static_string(ty: TokenType): string {
    return tok2str(*(&ty as *i32));
}

// returned boolean indicates if the strings has to be freed
def token_to_string(token: *Token, dest: *string): bool {
    // this is the range of tokens, that need to be formatted
    if token.ty >= TokenType.Int && token.ty <= TokenType.Ident {
        *dest = std.strndup(token.lexeme(), token.len());
        return true;
    }

    if token.ty == TokenType.String || token.ty == TokenType.Char {
        *dest = std.strndup(token.lexeme() - 1, token.len() + 2);
        return true;
    }

    *dest = token_ty_to_static_string(token.ty);
    return false;
}

def is_assign(ty: TokenType): bool {
    return ty >= TokenType.Eq && ty <= TokenType.CaretEq;
}

// Lexer

type Lexer struct {
    has_peek: bool,
    peek: Token,
    source: *source.SourceFile,
    start: string,
    current: string
}

def create(src: *source.SourceFile): Lexer {
    let span = span.create(src.code, src.code);
    let init_peek = Token { ty: TokenType.UnknownErr, span: span };

    return Lexer {
        has_peek: false,
        peek: init_peek,
        source: src,
        start: src.code,
        current: src.code
    };
}

def end_pos(l: *Lexer): usize {
    return l.source.len - 1;
}

def peek(l: *Lexer): Token {
    if l.has_peek {
        return l.peek;
    }

    l.peek = next_token(l);
    l.has_peek = true;
    return l.peek;
}

def next_token(l: *Lexer): Token {
    if l.has_peek {
        l.has_peek = false;
        return l.peek;
    }

    skip_whitespace(l);
    return get_next_token(l);
}

def current_pos(l: *Lexer): usize {
    if l.has_peek {
        return (l.peek.span.start - l.source.code) as usize;
    }

    return (l.current - l.source.code) as usize;
}

def current_ptr(l: *Lexer): string {
    if l.has_peek {
        return l.peek.span.start;
    }

    return l.current;
}

def current(l: *Lexer): i32 {
    let c = 0;
    let read_bytes = current_pos(l);
    std.read_char(l.current, l.source.len - read_bytes, &c);
    return c;
}

def source_file(l: *Lexer): *source.SourceFile {
    return l.source;
}

def is_num(c: i32): bool {
    return c >= '0' && c <= '9';
}

def is_letter(c: i32): bool {
    return c >= 'a' && c <= 'z'
        || c >= 'A' && c <= 'Z';
}

def check_keyword(l: *Lexer, start: usize, rest_len: usize, rest: string, ty: TokenType): TokenType {
    if (l.current - l.start) as usize == start + rest_len {
        if std.memcmp((l.start + start) as *void, rest as *void, rest_len) == 0 {
            return ty;
        }
    }

    return TokenType.Ident;
}

def ident_type(l: *Lexer): TokenType {
    let bytes_until = (l.start - l.source.code) as usize;
    let start = 0;
    let len = std.read_char(l.start, (l.source.len - bytes_until) as usize, &start) as usize;

    if start == 'l' {
        return check_keyword(l, 1, 2, "et", TokenType.Let);
    } else if start == 'i' && l.current - l.start > 1 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'f' {
            return check_keyword(l, 2, 0, "", TokenType.If);
        } else if next == 'm' {
            return check_keyword(l, 2, 4, "port", TokenType.Import);
        }
    } else if start == 'd' && l.current - l.start > 2 {
        let next = 0;
        len += std.read_char(l.start + len, l.source.len - bytes_until - len, &next) as usize;
        // read third char
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'f' {
            if l.current - l.start == 3 {
                return check_keyword(l, 1, 2, "ef", TokenType.Def);
            } else {
                return check_keyword(l, 1, 4, "efer", TokenType.Defer);
            }
        } else if next == 'l' {
            return check_keyword(l, 1, 5, "elete", TokenType.Delete);
        }
    } else if start == 'e' && l.current - l.start > 3 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'x' {
            return check_keyword(l, 2, 4, "tern", TokenType.Extern);
        } else if next == 'l' {
            return check_keyword(l, 2, 2, "se", TokenType.Else);
        } else if next == 'n' {
            return check_keyword(l, 2, 2, "um", TokenType.Enum);
        }
    } else if start == 'n' && l.current - l.start > 2 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'u' {
            return check_keyword(l, 2, 2, "ll", TokenType.Null);
        } else if next == 'e' {
            return check_keyword(l, 2, 1, "w", TokenType.New);
        }
    } else if start == 's' && l.current - l.start > 5 {
        let next = 0;
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 't' {
            return check_keyword(l, 2, 4, "ruct", TokenType.Struct);
        } else if next == 'i' {
            return check_keyword(l, 2, 4, "zeof", TokenType.Sizeof);
        }
    } else if start == 'u' && l.current - l.start > 4 {
        let next = 0;
        len += std.read_char(l.start + len, l.source.len - bytes_until - len, &next) as usize;
        // read third char
        std.read_char(l.start + len, l.source.len - bytes_until - len, &next);

        if next == 'i' {
            return check_keyword(l, 1, 4, "nion", TokenType.Union);
        } else if next == 'd' {
            return check_keyword(l, 1, 8, "ndefined", TokenType.Undefined);
        }
    } else if start == 'w' {
        return check_keyword(l, 1, 4, "hile", TokenType.While);
    } else if start == 't' {
        return check_keyword(l, 1, 3, "ype", TokenType.Type);
    } else if start == 'r' {
        return check_keyword(l, 1, 5, "eturn", TokenType.Return);
    } else if start == 'a' {
        return check_keyword(l, 1, 1, "s", TokenType.As);
    } else if start == 'f' {
        return check_keyword(l, 1, 2, "or", TokenType.For);
    } else if start == 'c' {
        return check_keyword(l, 1, 7, "ontinue", TokenType.Continue);
    } else if start == 'b' {
        return check_keyword(l, 1, 4, "reak", TokenType.Break);
    }

    return TokenType.Ident;
}

def lex_ident(l: *Lexer): Token {
    while is_letter(current(l)) || is_num(current(l)) || current(l) == '_' {
        advance(l);
    }

    return token_from_start(l, ident_type(l));
}

def lex_num(l: *Lexer): Token {
    while is_num(current(l)) {
        advance(l);
    }

    // float
    if current(l) == '.' {
        advance(l);
        while is_num(current(l)) {
            advance(l);
        }

        return token_from_start(l, TokenType.Float);
    }

    return token_from_start(l, TokenType.Int);
}

def lex_char_sequence(l: *Lexer, terminator: i32, ty: TokenType): Token {
    let backslash = 92;

    let error_start: string = null;
    let error_end: string = null;

    let should_continue = true;
    let escape_next = false;
    while !at_end(l) && should_continue {
        let curr = current(l);

        if curr == backslash && !escape_next {
            escape_next = true;
        } else if curr == terminator {
            should_continue = escape_next;
            escape_next = false;
        } else {
            if escape_next {
                // every escapable character
                if curr != '0'
                    && curr != 'n'
                    && curr != 'r'
                    && curr != 't'
                    && curr != '\\'
                    && curr != '\''
                    && curr != '"' {

                    error_start = l.current - 1;
                    error_end = l.current + 1;
                }
            }

            escape_next = false;
        }

        advance(l);
    }

    if error_start != null {
        return create_token(l, TokenType.EscapeErr, error_start, error_end);
    }

    return create_token(l, ty, l.start + 1, l.current - 1);
}

def lex_string(l: *Lexer): Token {
    return lex_char_sequence(l, '"', TokenType.String);
}

def lex_char(l: *Lexer): Token {
    let char_seq = lex_char_sequence(l, '\'', TokenType.Char);

    if char_seq.is_err() {
        return char_seq;
    }

    let len = l.current - l.start;
    if len == 4 && *(l.start + 1) == '\\' {
        return create_token(l, TokenType.Char, l.start + 1, l.current - 1);
    }

    if len != 3 {
        return create_token(l, TokenType.CharErr, l.start + 1, l.current - 1);
    }

    return create_token(l, TokenType.Char, l.start + 1, l.current - 1);
}

def is_whitespace(c: i32): bool {
    return c == ' '
        || c == '\r'
        || c == '\n'
        || c == '\t'
        || c == '\0';
}

def skip_until(l: *Lexer, ch: i32): void {
    while !at_end(l) {
        advance(l);
        if current(l) == ch {
            return;
        }
    }
}

def skip_whitespace(l: *Lexer): void {
    let should_continue = true;
    while !at_end(l) && should_continue {
        let c = current(l);
        should_continue = is_whitespace(c);
        if should_continue {
            advance(l);
        }
    }
}

def at_end(l: *Lexer): bool {
    return *l.current == '\0';
}

def advance(l: *Lexer): i32 {
    let read_bytes = current_pos(l);
    let c = 0;
    let len = std.read_char(l.current, (l.source.len - read_bytes) as usize, &c) as usize;

    l.current = l.current + len;
    return c;
}

def create_token(l: *Lexer, ty: TokenType, start: string, end: string): Token {
    return Token {
        ty: ty,
        span: span.create(start, end)
    };
}

def create_token_from_span(l: *Lexer, ty: TokenType, start: string, span: span.Span): Token {
    return Token {
        ty: ty,
        span: span
    };
}

def token_from_start(l: *Lexer, ty: TokenType): Token {
    return create_token(l, ty, l.start, l.current);
}

def get_next_token(l: *Lexer): Token {
    if at_end(l) {
        let ptr = current_ptr(l);
        return Token {
            ty: TokenType.EOF,
            span: span.create(ptr, ptr)
        };
    }

    l.start = l.current;
    let c = advance(l);

    if is_num(c) {
        return lex_num(l);
    }

    if is_letter(c) {
        return lex_ident(l);
    }

    if c == '"' {
        return lex_string(l);
    }

    if c == '\'' {
        return lex_char(l);
    }

    if c == ';' {
        return token_from_start(l, TokenType.Semi);
    }

    if c == ',' {
        return token_from_start(l, TokenType.Comma);
    }

    if c == ':' {
        return token_from_start(l, TokenType.Colon);
    }

    if c == '+' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.PlusEq);
        }
        return token_from_start(l, TokenType.Plus);
    }

    if c == '-' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.MinusEq);
        }
        return token_from_start(l, TokenType.Minus);
    }

    if c == '*' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.StarEq);
        }
        return token_from_start(l, TokenType.Star);
    }

    if c == '(' {
        return token_from_start(l, TokenType.LParen);
    }

    if c == ')' {
        return token_from_start(l, TokenType.RParen);
    }

    if c == '{' {
        return token_from_start(l, TokenType.LBrace);
    }

    if c == '}' {
        return token_from_start(l, TokenType.RBrace);
    }

    if c == '%' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.PercentEq);
        }
        return token_from_start(l, TokenType.Percent);
    }

    if c == '.' {
        if std.memcmp(l.start as *void, "..." as *void, 3) == 0 {
            advance(l);
            advance(l);
            return token_from_start(l, TokenType.TripleDot);
        }

        return token_from_start(l, TokenType.Dot);
    }

    if c == '/' {
        if current(l) == '/' {
            skip_until(l, '\n');
            return next_token(l);
        }
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.SlashEq);
        }
        return token_from_start(l, TokenType.Slash);
    }

    if c == '<' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.SmallerEq);
        }
        if current(l) == '<' {
            advance(l);
            return token_from_start(l, TokenType.SmallerSmaller);
        }
        return token_from_start(l, TokenType.Smaller);
    }

    if c == '>' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.GreaterEq);
        }
        if current(l) == '>' {
            advance(l);
            return token_from_start(l, TokenType.GreaterGreater);
        }
        return token_from_start(l, TokenType.Greater);
    }

    if c == '&' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.AmpersandEq);
        }
        if current(l) == '&' {
            advance(l);
            return token_from_start(l, TokenType.DoubleAmpersand);
        }
        return token_from_start(l, TokenType.Ampersand);
    }

    if c == '|' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.PipeEq);
        }
        if current(l) == '|' {
            advance(l);
            return token_from_start(l, TokenType.DoublePipe);
        }
        return token_from_start(l, TokenType.Pipe);
    }

    if c == '^' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.CaretEq);
        }
        return token_from_start(l, TokenType.Caret);
    }

    if c == '~' {
        return token_from_start(l, TokenType.Tilde);
    }

    if c == '=' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.DoubleEq);
        }
        return token_from_start(l, TokenType.Eq);
    }

    if c == '!' {
        if current(l) == '=' {
            advance(l);
            return token_from_start(l, TokenType.BangEq);
        }
        return token_from_start(l, TokenType.Bang);
    }


    return token_from_start(l, TokenType.UnknownErr);
}

