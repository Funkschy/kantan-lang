#include "./token_types.h"
#include "./expr_types.h"
#include "./error_code.h"
#include "./std.h"

#define CONSUME_RET_NULL(ty) if !consume(p, (ty)) { return null; } 0

import "lexer";
import "ast";
import "precedence";
import "span";
import "vec";
import "std";
import "mod";
import "param";

import "io"; // TODO: only debug

type ParseError struct {
    span: span.Span,
    text: string
}

type Parser struct {
    in_panic_mode: bool,
    lexer: lexer.Lexer,
    errors: vec.Vec // vector of ParseErrors
}

def create(source: string, span_interner: *span.SpanInterner, ctx: i32): Parser {
    return Parser {
        in_panic_mode: false,
        lexer: lexer.create(source, span_interner, ctx),
        errors: vec.create(sizeof ParseError)
    };
}

def free_p(p: *Parser): void {
    let i = 0;
    let elem = ParseError { span: span.empty(), text: null };
    while i < p.errors.len {
        vec.get(&p.errors, i, &elem as *void);
        delete elem.text;
        i = i + 1;
    }

    vec.free_v(&p.errors);
}

def num_errs(p: *Parser): i32 {
    return p.errors.len;
}

def get_err(p: *Parser, idx: i32, dest: *ParseError): bool {
    return vec.get(&p.errors, idx, dest as *void);
}

// advances to the next token and saves the new token into dest
// dest: can be left empty if the result is to be ignored
// returns: true, if no error occured, false otherwise
def advance(p: *Parser, dest: *lexer.Token): bool {
    let next = lexer.next_token(&p.lexer);

    if next.ty == TOKEN_EOF {
        if !p.in_panic_mode {
            let error = create_error(next.span, std.err2str(ERROR_UNEXPECTED_EOF));
            vec.push(&p.errors, &error as *void);
        }
        return false;
    } else if next.ty == TOKEN_ERR && !p.in_panic_mode {
        // advance should not care about lex errors while in panic mode
        let error = create_lex_error(p, &next);
        vec.push(&p.errors, &error as *void);
        return false;
    }

    if dest != null {
        *dest = next;
    }

    return true;
}

def consume(p: *Parser, ty: i32): bool {
    return consume_token(p, ty, null);
}

def consume_ident(p: *Parser, ident: *lexer.Token, allow_access_expr: bool): bool {
    if !consume_token(p, TOKEN_IDENT, ident) {
        return false;
    }

    if allow_access_expr {
        while peek(p).ty == TOKEN_DOT {
            consume(p, TOKEN_DOT);

            let first_part = *ident;
            if !consume_token(p, TOKEN_IDENT, ident) {
                return false;
            }

            let span = span.merge(p.lexer.span_interner, first_part.span, ident.span);
            *ident = lexer.create_token_from_span(
                &p.lexer,
                TOKEN_IDENT,
                first_part.lexeme,
                span
            );
        }
    }

    return true;
}

def consume_token(p: *Parser, ty: i32, dest: *lexer.Token): bool {
    let next = lexer.empty_token();
    if !advance(p, &next) {
        return false;
    }

    if dest != null {
        *dest = next;
    }

    if next.ty == ty {
        return true;
    }

    let error = create_consume_error(p, &next, ty);
    vec.push(&p.errors, &error as *void);
    return false;
}

def peek(p: *Parser): lexer.Token {
    return lexer.peek(&p.lexer);
}

def parse(p: *Parser): mod.Module {
    let mod = mod.create();

    while peek(p).ty != TOKEN_EOF {
        let s = statement(p);
        if s != null {
            mod.push_stmt(&mod, s);
        }
    }

    return mod;
}

def statement(p: *Parser): *ast.Stmt {
    let num_errs = num_errs(p);
    let start = lexer.current_pos(&p.lexer);

    let s: *ast.Stmt = null;
    let consume_semi = parse_statement(p, &s);

    if s == null {
        sync(p);

        // no parse error was added...
        if num_errs == num_errs(p) {
            // ... so just add a generic one
            let end = lexer.current_pos(&p.lexer);
            let error = create_statement_error(p, start, end);
            vec.push(&p.errors, &error as *void);
        }

        return null;
    }

    if consume_semi {
        if !consume(p, TOKEN_SEMI) {
            sync(p);
            ast.stmt_free(s);
            return null;
        }
    }

    return s;
}

def parse_statement(p: *Parser, dest: **ast.Stmt): bool {
    let peek = peek(p);
    if peek.ty == TOKEN_LET {
        *dest = parse_let_stmt(p);
        return true;
    } else if peek.ty == TOKEN_IMPORT {
        *dest = parse_import_stmt(p);
        return true;
    } else if peek.ty == TOKEN_DEF {
        *dest = parse_func_def(p);
        return false;
    }

    let expr = expression(p);
    if expr == null {
        *dest = null;
        return true;
    }

    *dest = ast.make_expr_stmt(expr);
    return true;
}

def parse_import_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_IMPORT);

    let mod_path = lexer.empty_token();
    if !consume_token(p, TOKEN_STRING, &mod_path) {
        return null;
    }

    return ast.make_import_stmt(mod_path.span, mod_path.lexeme);
}

def parse_let_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_LET);

    let ident = lexer.empty_token();
    if !consume_ident(p, &ident, false) {
        return null;
    }

    CONSUME_RET_NULL(TOKEN_EQ);

    let e = expression(p);
    if e == null {
        // TODO: create error
        return null;
    }
    return ast.make_let_stmt(ident, e);
}

def parse_func_def(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_DEF);

    let ident = lexer.empty_token();
    if !consume_ident(p, &ident, false) {
        return null;
    }

    CONSUME_RET_NULL(TOKEN_LPAREN);
    let params = parse_param_list(p, TOKEN_COMMA, TOKEN_RPAREN);
    let param_list = func.pl_from_vec(params);

    if !consume(p, TOKEN_COLON) {
        func.free_pl(&param_list);
        return null;
    }

    let ret_ty = lexer.empty_token();
    if !consume_ident(p, &ret_ty, true) {
        func.free_pl(&param_list);
        return null;
    }

    let block = parse_block(p);
    if block == null {
        func.free_pl(&param_list);
        return null;
    }

    return ast.make_func_decl_stmt(
        ident,
        param_list,
        ident.create(ret_ty),
        ast.as_block_stmt(block)
    );
}

def free_statements(statements: *ptrvec.Vec): void {
    let i = 0;
    while i < statements.len {
        let s = ptrvec.get(statements, i) as *ast.Stmt;
        ast.stmt_free(s);
        i = i + 1;
    }

    ptrvec.free_v(statements);
}

def parse_block(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_LBRACE);
    let statements = ptrvec.create();

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != TOKEN_RBRACE {
        let s = statement(p);
        if s == null {
            free_statements(&statements);
            return null;
        }

        ptrvec.push_ptr(&statements, s as *void);
        peek = peek(p);
    }

    if !consume(p, TOKEN_RBRACE) {
        free_statements(&statements);
        return null;
    }

    return ast.make_block_stmt(statements);
}

def parse_param_list(p: *Parser, sep_ty: i32, delim_ty: i32): vec.Vec {
#define FAIL() vec.free_v(&params); return vec.create(0)

    let params = vec.create(sizeof param.Param);

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != delim_ty {
        let ident = lexer.empty_token();
        if !consume_ident(p, &ident, false) {
            FAIL();
        }
        let ident = ident.create(ident);

        if !consume(p, TOKEN_COLON) {
            FAIL();
        }

        let ty = lexer.empty_token();
        if !consume_ident(p, &ty, true) {
            FAIL();
        }
        let ty = ident.create(ty);

        let param = param.create(ident, ty);
        vec.push(&params, &param as *void);

        peek = peek(p);

        // consume separator
        if peek.ty == sep_ty {
            if !consume(p, sep_ty) {
                FAIL();
            }
        }
    }

    if !consume(p, delim_ty) {
        FAIL();
    }

    return params;

#undef FAIL
}

def expression(p: *Parser): *ast.Expr {
    return parse_expression(p, precedence.assign());
}

def parse_expression(p: *Parser, precedence: i32): *ast.Expr {
    letcheck(left, prefix(p));

    while next_higher_precedence(p, precedence) {
        let token = lexer.empty_token();
        if !advance(p, &token) {
            delete left;
            return null;
        }

        let infix = infix(p, &token, left);
        if infix == null {
            delete left;
            return null;
        }

        left = infix;
    }

    return left;
}

def next_higher_precedence(p: *Parser, precedence: i32): bool {
    let t = peek(p);

    if t.ty == TOKEN_ERR || t.ty == TOKEN_EOF {
        return false;
    }

    return precedence.get(t.ty) > precedence;
}

def prefix(p: *Parser): *ast.Expr {
    let token = lexer.empty_token();
    if !advance(p, &token) {
        return null;
    }

    if token.ty == TOKEN_INT {
        let len = span.get(p.lexer.span_interner, token.span).len;
        return ast.make_int(token.span, len, token.lexeme);
    } else if token.ty == TOKEN_STRING {
        let len = span.get(p.lexer.span_interner, token.span).len;
        return ast.make_string(token.span, len, token.lexeme);
    } else if token.ty == TOKEN_LPAREN {
        letcheck(e, expression(p));

        if !consume(p, TOKEN_RPAREN) {
            ast.expr_free(e);
            return null;
        }

        // TODO: this can over/underflow. Needs to be checked for interning
        e.span.start = e.span.start - 1;
        e.span.len_or_tag = e.span.len_or_tag + 2;

        return e;
    }

    return null;
}

def infix(p: *Parser, token: *lexer.Token, left: *ast.Expr): *ast.Expr {
    let found = false;
    let ty = 0;

    if token.ty == TOKEN_PLUS {
        found = true;
        ty = BINARY_ADD;
    } else if token.ty == TOKEN_MINUS {
        found = true;
        ty = BINARY_SUB;
    } else if token.ty == TOKEN_STAR {
        found = true;
        ty = BINARY_MUL;
    } else if token.ty == TOKEN_SLASH {
        found = true;
        ty = BINARY_DIV;
    }

    // a pattern matched
    if found {
        let prec = precedence.get(token.ty);
        letcheck(right, parse_expression(p, prec));
        let span = span.merge(p.lexer.span_interner, left.span, right.span);
        return ast.make_binary(span, ty, left, right);
    }

    return null;
}

def sync(p: *Parser): void {
    p.in_panic_mode = true;
    inner_sync(p);
    p.in_panic_mode = false;
}

// should only be called by sync function
def inner_sync(p: *Parser): void {
    let previous = lexer.empty_token();
    if !advance(p, &previous) {
        return;
    }

    let peek = peek(p);
    while peek.ty != TOKEN_EOF {
        if previous.ty == TOKEN_SEMI {
            return;
        }

        if peek.ty == TOKEN_LET || peek.ty == TOKEN_DEF {
            return;
        }

        if !advance(p, &previous) {
            return;
        }
        peek = peek(p);
    }
}

def create_error(span: span.Span, text: string): ParseError {
    return ParseError { span: span, text: text };
}

def create_statement_error(p: *Parser, start: i32, end: i32): ParseError {
    let span = lexer.create_span(&p.lexer, start, end);
    let len = span.get(p.lexer.span_interner, span).len;

    return create_error(span, std.err2str(ERROR_COULD_NOT_PARSE_STMT, len, p.lexer.source + start));
}

def create_lex_error(p: *Parser, next: *lexer.Token): ParseError {
    // len for lexeme
    let len = span.get(p.lexer.span_interner, next.span).len;

    return create_error(
        next.span,
        std.err2str(ERROR_UNKNOWN_SYMBOL, len, next.lexeme)
    );
}

def create_consume_error(p: *Parser, peek: *lexer.Token, expected_ty: i32): ParseError {
    let peek_str: string = null;
    let needs_free = lexer.token_to_string(peek, p.lexer.span_interner, &peek_str);

    let err_str = std.err2str(
        ERROR_EXPECTED_BUT_GOT,
        lexer.token_ty_to_static_string(expected_ty),
        peek_str
    );

    if needs_free {
        delete peek_str;
    }

    return create_error(peek.span, err_str);
}

