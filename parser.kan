#include "./token_types.h"
#include "./expr_types.h"
#include "./error_code.h"
#include "./std.h"

#define CONSUME_RET_NULL(ty) if !consume(p, (ty)) { return null; } 0

import "lexer";
import "ast";
import "precedence";
import "span";
import "vec";
import "std";
import "mod";
import "param";
import "tyid";

import "io"; // TODO: only debug

type ParseError struct {
    span: span.Span,
    text: string
}

def empty_err(): ParseError {
    return ParseError { span: span.empty(), text: null };
}

type Parser struct {
    in_panic_mode: bool,
    lexer: lexer.Lexer,
    errors: vec.Vec // vector of ParseErrors
}

def create(source: string, span_interner: *span.SpanInterner, ctx: i32): Parser {
    return Parser {
        in_panic_mode: false,
        lexer: lexer.create(source, span_interner, ctx),
        errors: vec.create(sizeof ParseError)
    };
}

def free_p(p: *Parser): void {
    free_errs_from(p, 0);
    vec.free_v(&p.errors);
}

def free_errs_from(p: *Parser, start: i32): void {
    let i = start;
    let elem = empty_err();
    while i < p.errors.len {
        vec.get(&p.errors, i, &elem as *void);
        delete elem.text;
        i = i + 1;
    }
    p.errors.len = p.errors.len - (p.errors.len - start);
}

def num_errs(p: *Parser): i32 {
    return p.errors.len;
}

def get_err(p: *Parser, idx: i32, dest: *ParseError): bool {
    return vec.get(&p.errors, idx, dest as *void);
}

def push_err(p: *Parser, error: ParseError): void {
    vec.push(&p.errors, &error as *void);
}

// advances to the next token and saves the new token into dest
// dest: can be left empty if the result is to be ignored
// returns: true, if no error occured, false otherwise
def advance(p: *Parser, dest: *lexer.Token): bool {
    let next = lexer.next_token(&p.lexer);

    if next.ty == TOKEN_EOF {
        if !p.in_panic_mode {
            let error = create_error(next.span, std.err2str(ERROR_UNEXPECTED_EOF));
            vec.push(&p.errors, &error as *void);
        }
        return false;
    } else if lexer.is_err(&next) && !p.in_panic_mode {
        // advance should not care about lex errors while in panic mode
        let error = create_lex_error(p, &next);
        push_err(p, error);
        return false;
    }

    if dest != null {
        *dest = next;
    }

    return true;
}

def consume(p: *Parser, ty: i32): bool {
    return consume_token(p, ty, null);
}

def consume_ident(p: *Parser, ident: *lexer.Token, allow_access_expr: bool): bool {
    if !consume_token(p, TOKEN_IDENT, ident) {
        return false;
    }

    if allow_access_expr {
        while peek(p).ty == TOKEN_DOT {
            consume(p, TOKEN_DOT);

            let first_part = *ident;
            if !consume_token(p, TOKEN_IDENT, ident) {
                return false;
            }

            let span = span.merge(p.lexer.span_interner, first_part.span, ident.span);
            *ident = lexer.create_token_from_span(
                &p.lexer,
                TOKEN_IDENT,
                first_part.lexeme,
                span
            );
        }
    }

    return true;
}

def consume_token(p: *Parser, ty: i32, dest: *lexer.Token): bool {
    let next = lexer.empty_token();
    if !advance(p, &next) {
        return false;
    }

    if dest != null {
        *dest = next;
    }

    if next.ty == ty {
        return true;
    }

    let error = create_consume_error(p, &next, ty);
    push_err(p, error);
    return false;
}

def consume_type(p: *Parser, dest: *tyid.Type): bool {
    let num_errs = num_errs(p);

    let pointer_count = 0;
    while peek(p).ty == TOKEN_STAR {
        consume(p, TOKEN_STAR);
        pointer_count = pointer_count + 1;
    }

    let ty = lexer.empty_token();
    if !consume_ident(p, &ty, true) {
        if num_errs(p) > num_errs {
            free_errs_from(p, num_errs);
            let error = create_consume_error_text(p, &ty, "type");
            push_err(p, error);
        }

        return false;
    }
    *dest = tyid.create_ptr(ident.create(ty), pointer_count);
    return true;
}

def peek(p: *Parser): lexer.Token {
    return lexer.peek(&p.lexer);
}

def parse(p: *Parser): mod.Module {
    let mod = mod.create();

    while peek(p).ty != TOKEN_EOF {
        let s = statement(p);
        if s != null {
            mod.push_stmt(&mod, s);
        }
    }

    return mod;
}

def statement(p: *Parser): *ast.Stmt {
    let num_errs = num_errs(p);
    let start = lexer.current_pos(&p.lexer);

    let s: *ast.Stmt = null;
    let consume_semi = parse_statement(p, &s);

    if s == null {
        let end = sync(p);

        // no parse error was added...
        if num_errs == num_errs(p) {
            // ... so just add a generic one
            let error = create_statement_error(p, start, end);
            push_err(p, error);
        }

        return null;
    }

    if consume_semi {
        if !consume(p, TOKEN_SEMI) {
            sync(p);
            ast.stmt_free(s);
            return null;
        }
    }

    return s;
}

def parse_statement(p: *Parser, dest: **ast.Stmt): bool {
    let peek = peek(p);

    if peek.ty == TOKEN_DEF {
        *dest = parse_func_def(p, false);
        return false;
    }

    if peek.ty == TOKEN_IF {
        *dest = parse_if_stmt(p);
        return false;
    }

    if peek.ty == TOKEN_WHILE {
        *dest = parse_while_stmt(p);
        return false;
    }

    if peek.ty == TOKEN_TYPE {
        *dest = parse_type_stmt(p);
        return false;
    }

    if peek.ty == TOKEN_EXTERN {
        consume(p, TOKEN_EXTERN);
        *dest = parse_func_def(p, true);
        return true;
    }

    if peek.ty == TOKEN_DELETE {
        *dest = parse_delete_stmt(p);
        return true;
    }

    if peek.ty == TOKEN_RETURN {
        *dest = parse_return_stmt(p);
        return true;
    }

    if peek.ty == TOKEN_LET {
        *dest = parse_let_stmt(p);
        return true;
    }

    if peek.ty == TOKEN_IMPORT {
        *dest = parse_import_stmt(p);
        return true;
    }

    let expr = expression(p, false);
    if expr == null {
        *dest = null;
        return true;
    }

    *dest = ast.new_expr_stmt(expr);
    return true;
}

def parse_delete_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_DELETE);

    letcheck(expr, expression(p, false));
    return ast.new_delete_stmt(expr);
}

def parse_return_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_RETURN);

    if peek(p).ty == TOKEN_SEMI {
        return ast.new_return_stmt(null);
    }

    letcheck(expr, expression(p, false));
    return ast.new_return_stmt(expr);
}

def parse_import_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_IMPORT);

    let mod_path = lexer.empty_token();
    if !consume_token(p, TOKEN_STRING, &mod_path) {
        return null;
    }

    return ast.new_import_stmt(mod_path.span, mod_path.lexeme);
}

def parse_let_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_LET);

    let ident = lexer.empty_token();
    if !consume_ident(p, &ident, false) {
        return null;
    }

    let ty = tyid.empty();
    if peek(p).ty == TOKEN_COLON {
        CONSUME_RET_NULL(TOKEN_COLON);
        if !consume_type(p, &ty) {
            return null;
        }
    }

    CONSUME_RET_NULL(TOKEN_EQ);

    let e = expression(p, false);
    if e == null {
        // TODO: create error
        return null;
    }
    return ast.new_let_stmt(ident, ty, e);
}

def parse_type_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_TYPE);

    let ident = lexer.empty_token();
    if !consume_ident(p, &ident, false) {
        return null;
    }

    CONSUME_RET_NULL(TOKEN_STRUCT);
    CONSUME_RET_NULL(TOKEN_LBRACE);
    let fields = parse_field_list(p);
    if fields.elem_size == 0 {
        vec.free_v(&fields);
        return null;
    }

    return ast.new_struct_decl_stmt(
        p.lexer.span_interner,
        ident.create(ident),
        fields
    );
}

def parse_func_def(p: *Parser, is_extern: bool): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_DEF);

    let ident = lexer.empty_token();
    if !consume_ident(p, &ident, false) {
        return null;
    }

    CONSUME_RET_NULL(TOKEN_LPAREN);
    let params = parse_param_list(p, is_extern);
    let param_list = func.pl_from_vec(params);

    if !consume(p, TOKEN_COLON) {
        func.free_pl(&param_list);
        return null;
    }

    let ret_ty = tyid.empty();
    if !consume_type(p, &ret_ty) {
        func.free_pl(&param_list);
        return null;
    }

    let block: *ast.Stmt = null;
    if !is_extern {
        block = parse_block(p);
        if block == null {
            func.free_pl(&param_list);
            return null;
        }
    }

    return ast.new_func_decl_stmt(
        ident,
        param_list,
        ret_ty,
        is_extern,
        ast.as_block_stmt(block)
    );
}

def parse_while_stmt(p: *Parser): *ast.Stmt {
    if !consume(p, TOKEN_WHILE) {
        return null;
    }

    let peek = peek(p);
    let condition = expression(p, true);

    if condition == null {
        let error = create_consume_error_text(p, &peek, "condition");
        push_err(p, error);
        return null;
    }

    let block = parse_block(p);
    if block == null {
        ast.expr_free(condition);
        return null;
    }

    return ast.new_while_stmt(condition, ast.as_block_stmt(block));
}

def parse_if_stmt(p: *Parser): *ast.Stmt {
    if !consume(p, TOKEN_IF) {
        return null;
    }

    let peek = peek(p);
    let condition = expression(p, true);

    if condition == null {
        let error = create_consume_error_text(p, &peek, "condition");
        push_err(p, error);
        return null;
    }

    let block = parse_block(p);
    if block == null {
        ast.expr_free(condition);
        return null;
    }

    let if_stmt = ast.new_if_stmt(condition, ast.as_block_stmt(block), null);
    if peek(p).ty != TOKEN_ELSE {
        return if_stmt;
    }
    consume(p, TOKEN_ELSE);

    let else_stmt: *ast.Stmt = null;
    let peek = peek(p);
    if peek.ty == TOKEN_IF {
        else_stmt = parse_if_stmt(p);
    } else if peek.ty == TOKEN_LBRACE {
        else_stmt = parse_block(p);
    } else {
        let error = create_consume_error_text(p, &peek, "either 'if' or '{' after else");
        push_err(p, error);
    }

    if else_stmt == null {
        ast.stmt_free(if_stmt);
        return null;
    }

    ast.as_if_stmt(if_stmt).else_stmt = else_stmt;
    return if_stmt;
}

def free_statements(statements: *ptrvec.Vec): void {
    let i = 0;
    while i < statements.len {
        let s = ptrvec.get(statements, i) as *ast.Stmt;
        ast.stmt_free(s);
        i = i + 1;
    }

    ptrvec.free_v(statements);
}

def parse_block(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_LBRACE);
    let statements = ptrvec.create();

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != TOKEN_RBRACE {
        let s = statement(p);
        if s == null {
            free_statements(&statements);
            return null;
        }

        ptrvec.push_ptr(&statements, s as *void);
        peek = peek(p);
    }

    if !consume(p, TOKEN_RBRACE) {
        free_statements(&statements);
        return null;
    }

    return ast.new_block_stmt(statements);
}

def parse_field_list(p: *Parser): vec.Vec {
#define FAIL() vec.free_v(&fields); return vec.create(0)

    let fields = vec.create(sizeof record.Field);

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != TOKEN_RBRACE {
        let ident = lexer.empty_token();
        if !consume_ident(p, &ident, false) {
            FAIL();
        }
        let ident = ident.create(ident);

        if !consume(p, TOKEN_COLON) {
            FAIL();
        }

        let ty = tyid.empty();
        if !consume_type(p, &ty) {
            FAIL();
        }

        let field = record.create_field(ident, ty);
        vec.push(&fields, &field as *void);

        peek = peek(p);

        // consume separator
        if peek.ty == TOKEN_COMMA {
            consume(p, TOKEN_COMMA);
        }
    }

    if !consume(p, TOKEN_RBRACE) {
        FAIL();
    }

    return fields;

#undef FAIL
}


def parse_param_list(p: *Parser, allow_va: bool): vec.Vec {
#define FAIL() vec.free_v(&params); return vec.create(0)

    let params = vec.create(sizeof param.Param);

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != TOKEN_RPAREN {
        let ident = lexer.empty_token();
        let ty = tyid.empty();

        if allow_va && peek(p).ty == TOKEN_TRIPLE_DOT {
            consume_token(p, TOKEN_TRIPLE_DOT, &ident);
        } else {
            ident = lexer.empty_token();
            if !consume_ident(p, &ident, false) {
                FAIL();
            }

            if !consume(p, TOKEN_COLON) {
                FAIL();
            }

            if !consume_type(p, &ty) {
                FAIL();
            }
        }

        let ident = ident.create(ident);
        let param = param.create(ident, ty);
        vec.push(&params, &param as *void);

        peek = peek(p);

        // consume separator
        if peek.ty == TOKEN_COMMA {
            consume(p, TOKEN_COMMA);
        }
    }

    if !consume(p, TOKEN_RPAREN) {
        FAIL();
    }

    return params;

#undef FAIL
}

def parse_arg_list(p: *Parser): ast.ArgList {
#define FAIL() ast.free_al(&args); return ast.create_arg_list()
    let args = ast.create_arg_list();

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != TOKEN_RPAREN {
        let arg = expression(p, false);
        if arg == null {
            FAIL();
        }
        ast.push_arg(&args, arg);

        peek = peek(p);

        if peek.ty == TOKEN_COMMA {
            consume(p, TOKEN_COMMA);
        }
    }

    return args;
#undef FAIL
}

def parse_init_list(p: *Parser): ast.InitList {
#define FAIL() ast.free_il(&inits); return ast.create_init_list()
    let inits = ast.create_init_list();

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != TOKEN_RBRACE {
        let ident = lexer.empty_token();
        if !consume_ident(p, &ident, false) {
            FAIL();
        }

        if !consume(p, TOKEN_COLON) {
            FAIL();
        }

        let val = expression(p, false);
        if val == null {
            FAIL();
        }

        let init = ast.create_init(ident.create(ident), val);
        ast.push_init(&inits, init);

        peek = peek(p);

        if peek.ty == TOKEN_COMMA {
            consume(p, TOKEN_COMMA);
        }
    }

    return inits;
#undef FAIL
}

def expression(p: *Parser, no_struct: bool): *ast.Expr {
    letcheck(left, parse_expression(p, precedence.assign(), no_struct));
    while peek(p).ty == TOKEN_EQ {
        consume(p, TOKEN_EQ);
        let value = expression(p, no_struct);
        if value == null {
            ast.expr_free(left);
            return null;
        }
        let span = span.merge(p.lexer.span_interner, left.span, value.span);
        left = ast.new_assign_expr(span, left, value);
    }

    return left;
}

def parse_expression(p: *Parser, precedence: i32, no_struct: bool): *ast.Expr {
    letcheck(left, prefix(p, no_struct));

    while next_higher_precedence(p, precedence, no_struct) {
        let token = lexer.empty_token();
        if !advance(p, &token) {
            ast.expr_free(left);
            return null;
        }

        let infix = infix(p, &token, left, no_struct);
        if infix == null {
            ast.expr_free(left);
            return null;
        }

        left = infix;
    }

    return left;
}

def next_higher_precedence(p: *Parser, precedence: i32, no_struct: bool): bool {
    let t = peek(p);

    if lexer.is_err(&t) || t.ty == TOKEN_EOF {
        return false;
    }

    if t.ty == TOKEN_LBRACE {
        return !no_struct && precedence.get(t.ty) > precedence;
    }

    return precedence.get(t.ty) > precedence;
}

def prefix(p: *Parser, no_struct: bool): *ast.Expr {
#define UNARY(tok, unary_ty) if token.ty == (tok) { \
        letcheck(expr, parse_expression(p, precedence.unary(), no_struct)); \
        let span = span.merge(p.lexer.span_interner, token.span, expr.span); \
        return ast.new_unary_expr(span, (unary_ty), expr); \
    } 0

    let token = lexer.empty_token();
    if !advance(p, &token) {
        return null;
    }

    if token.ty == TOKEN_INT {
        return ast.new_int_expr(token.span, token.lexeme);
    }

    if token.ty == TOKEN_STRING {
        return ast.new_string_expr(token.span, token.lexeme);
    }

    if token.ty == TOKEN_CHAR {
        return ast.new_char_expr(token.span, token.lexeme);
    }

    if token.ty == TOKEN_IDENT {
        return ast.new_ident_expr(token);
    }

    if token.ty == TOKEN_NEW {
        letcheck(e, expression(p, no_struct));
        let span = span.merge(p.lexer.span_interner, token.span, e.span);
        return ast.new_new_expr(span, e);
    }

    UNARY(TOKEN_AMPERSAND, UNARY_REF);
    UNARY(TOKEN_STAR, UNARY_DEREF);
    UNARY(TOKEN_BANG, UNARY_NEG_BOOL);
    UNARY(TOKEN_MINUS, UNARY_NEG_NUM);

    if token.ty == TOKEN_LPAREN {
        // ignore no_struct, because it's in parens
        letcheck(e, expression(p, false));

        if !consume(p, TOKEN_RPAREN) {
            ast.expr_free(e);
            return null;
        }

        // TODO: this can over/underflow. Needs to be checked for interning
        e.span.start = e.span.start - 1;
        e.span.len_or_tag = e.span.len_or_tag + 2;

        return e;
    }

    return null;
#undef UNARY
}

def infix(p: *Parser, token: *lexer.Token, left: *ast.Expr, no_struct: bool): *ast.Expr {
    // access expr
    if token.ty == TOKEN_DOT {
        letcheck(right, expression(p, no_struct));
        let span = span.merge(p.lexer.span_interner, left.span, right.span);
        return ast.new_access_expr(span, left, right);
    }

    // call expr
    if token.ty == TOKEN_LPAREN {
        let args = parse_arg_list(p);

        let closing_paren = lexer.empty_token();
        if !consume_token(p, TOKEN_RPAREN, &closing_paren) {
            ast.free_al(&args);
            return null;
        }

        let span = span.merge(p.lexer.span_interner, left.span, closing_paren.span);
        return ast.new_call_expr(span, left, args);
    }

    // init expr
    if token.ty == TOKEN_LBRACE {
        let inits = parse_init_list(p);

        let closing_brace = lexer.empty_token();
        if !consume_token(p, TOKEN_RBRACE, &closing_brace) {
            ast.free_il(&inits);
            return null;
        }

        let span = span.merge(p.lexer.span_interner, left.span, closing_brace.span);
        return ast.new_init_expr(span, left, inits);
    }

    if token.ty == TOKEN_AS {
        let ty = tyid.empty();
        if !consume_type(p, &ty) {
            return null;
        }

        let span = span.merge(p.lexer.span_interner, left.span, tyid.span(&ty));
        return ast.new_as_expr(span, left, ty);
    }

    // binary expression

    let found = false;
    let ty = 0;

    if token.ty == TOKEN_PLUS {
        found = true;
        ty = BINARY_ADD;
    } else if token.ty == TOKEN_MINUS {
        found = true;
        ty = BINARY_SUB;
    } else if token.ty == TOKEN_STAR {
        found = true;
        ty = BINARY_MUL;
    } else if token.ty == TOKEN_SLASH {
        found = true;
        ty = BINARY_DIV;
    } else if token.ty == TOKEN_PERCENT {
        found = true;
        ty = BINARY_MOD;
    } else if token.ty == TOKEN_SMALLER {
        found = true;
        ty = BINARY_ST;
    } else if token.ty == TOKEN_SMALLER_EQ {
        found = true;
        ty = BINARY_SE;
    } else if token.ty == TOKEN_GREATER {
        found = true;
        ty = BINARY_GT;
    } else if token.ty == TOKEN_GREATER_EQ {
        found = true;
        ty = BINARY_GE;
    } else if token.ty == TOKEN_DOUBLE_AMPERSAND {
        found = true;
        ty = BINARY_LOG_AND;
    } else if token.ty == TOKEN_DOUBLE_PIPE {
        found = true;
        ty = BINARY_LOG_OR;
    } else if token.ty == TOKEN_DOUBLE_EQ {
        found = true;
        ty = BINARY_EQ;
    } else if token.ty == TOKEN_BANG_EQ {
        found = true;
        ty = BINARY_NE;
    }

    // a pattern matched
    if found {
        let prec = precedence.get(token.ty);
        letcheck(right, parse_expression(p, prec, no_struct));
        let span = span.merge(p.lexer.span_interner, left.span, right.span);
        return ast.new_binary_expr(span, ty, left, right);
    }

    return null;
}

def sync(p: *Parser): i32 {
    p.in_panic_mode = true;
    let pos = inner_sync(p);
    p.in_panic_mode = false;
    return pos;
}

// should only be called by sync function
def inner_sync(p: *Parser): i32 {
    let previous = lexer.empty_token();
    if !advance(p, &previous) {
        return lexer.end_pos(&p.lexer);
    }

    let peek = peek(p);
    while peek.ty != TOKEN_EOF {
        let span = span.get(p.lexer.span_interner, previous.span);
        let pos = span.start + span.len;
        if previous.ty == TOKEN_SEMI && peek(p).ty != TOKEN_RBRACE {
            return pos;
        }

        if peek.ty == TOKEN_LET || peek.ty == TOKEN_DEF {
            return pos;
        }

        if !advance(p, &previous) {
            return pos;
        }

        if previous.ty == TOKEN_RBRACE {
            let span = span.get(p.lexer.span_interner, previous.span);
            return span.start + span.len;
        }

        peek = peek(p);
    }

    let span = span.get(p.lexer.span_interner, previous.span);
    return span.start + span.len;
}

def create_error(span: span.Span, text: string): ParseError {
    return ParseError { span: span, text: text };
}

def create_statement_error(p: *Parser, start: i32, end: i32): ParseError {
    let span = lexer.create_span(&p.lexer, start, end);
    let len = span.get(p.lexer.span_interner, span).len;

    return create_error(span, std.err2str(ERROR_COULD_NOT_PARSE_STMT, len, p.lexer.source + start));
}

def create_lex_error(p: *Parser, next: *lexer.Token): ParseError {
    // len for lexeme
    let len = lexer.token_len(next, p.lexer.span_interner);
    let err_ty = ERROR_UNKNOWN_SYMBOL;

    if next.ty == TOKEN_CHAR_ERR {
        err_ty = ERROR_CHAR_LIT_LEN;
    }

    return create_error(
        next.span,
        std.err2str(err_ty, len, next.lexeme)
    );
}

def create_consume_error(p: *Parser, peek: *lexer.Token, expected_ty: i32): ParseError {
    return create_consume_error_text(
        p,
        peek,
        lexer.token_ty_to_static_string(expected_ty)
    );
}

def create_consume_error_text(p: *Parser, peek: *lexer.Token, text: string): ParseError {
    let peek_str: string = null;
    let needs_free = lexer.token_to_string(peek, p.lexer.span_interner, &peek_str);

    let err_str = std.err2str(
        ERROR_EXPECTED_BUT_GOT,
        text,
        peek_str
    );

    if needs_free {
        delete peek_str;
    }

    return create_error(peek.span, err_str);
}
