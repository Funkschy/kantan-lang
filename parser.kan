#include "./token_types.h"
#include "./expr_types.h"
#include "./error_code.h"
#include "./std.h"

import "lexer";
import "ast";
import "precedence";
import "span";
import "vec";
import "std";

import "io"; // TODO: only debug

type ParseError struct {
    span: span.Span,
    text: string
}

type Parser struct {
    in_panic_mode: bool,
    lexer: lexer.Lexer,
    errors: vec.Vec // vector of ParseErrors
}

def create(source: string, span_interner: *span.SpanInterner, ctx: i32): Parser {
    return Parser {
        in_panic_mode: false,
        lexer: lexer.create(source, span_interner, ctx),
        errors: vec.create(sizeof ParseError)
    };
}

def free_p(p: *Parser): void {
    let i = 0;
    let elem = ParseError { span: span.empty(), text: null };
    while i < p.errors.len {
        vec.get(&p.errors, i, &elem as *void);
        delete elem.text;
        i = i + 1;
    }

    vec.free_v(&p.errors);
}

def num_errs(p: *Parser): i32 {
    return p.errors.len;
}

def get_err(p: *Parser, idx: i32, dest: *ParseError): bool {
    return vec.get(&p.errors, idx, dest as *void);
}

// advances to the next token and saves the new token into dest
// dest: can be left empty if the result is to be ignored
// returns: true, if no error occured, false otherwise
def advance(p: *Parser, dest: *lexer.Token): bool {
    let next = lexer.next_token(&p.lexer);

    if next.ty == TOKEN_EOF {
        if !p.in_panic_mode {
            let error = create_error(next.span, std.err2str(ERROR_UNEXPECTED_EOF));
            vec.push(&p.errors, &error as *void);
        }
        return false;
    } else if next.ty == TOKEN_ERR && !p.in_panic_mode {
        // advance should not care about lex errors while in panic mode
        let error = create_lex_error(p, &next);
        vec.push(&p.errors, &error as *void);
        return false;
    }

    if dest != null {
        *dest = next;
    }

    return true;
}

def consume(p: *Parser, ty: i32): bool {
    return consume_token(p, ty, null);
}

def consume_token(p: *Parser, ty: i32, dest: *lexer.Token): bool {
    let next = lexer.empty_token();
    if !advance(p, &next) {
        return false;
    }

    if dest != null {
        *dest = next;
    }

    if next.ty == ty {
        return true;
    }

    let error = create_consume_error(p, &next, ty);
    vec.push(&p.errors, &error as *void);
    return false;
}

def peek(p: *Parser): lexer.Token {
    return lexer.peek(&p.lexer);
}

def parse(p: *Parser): ast.Module {
    let mod = ast.mod_create();

    while peek(p).ty != TOKEN_EOF {
        let s = statement(p);
        if s != null {
            if !consume(p, TOKEN_SEMI) {
                sync(p);
                ast.stmt_free(s);
            } else {
                ast.mod_push(&mod, s);
            }
        }
    }

    return mod;
}

def statement(p: *Parser): *ast.Stmt {
    let num_errs = num_errs(p);
    let start = lexer.current_pos(&p.lexer);

    let s = parse_statement(p);
    if s == null {
        sync(p);

        // no parse error was added...
        if num_errs == num_errs(p) {
            // ... so just add a generic one
            let end = lexer.current_pos(&p.lexer);
            let error = create_statement_error(p, start, end);
            vec.push(&p.errors, &error as *void);
        }

        return null;
    }

    return s;
}

def parse_statement(p: *Parser): *ast.Stmt {
    let peek = peek(p);
    if peek.ty == TOKEN_LET {
        return parse_let_stmt(p);
    } else if peek.ty == TOKEN_IMPORT {
        return parse_import_stmt(p);
    }

    letcheck(expr, expression(p));
    return ast.make_expr_stmt(expr);
}

def parse_import_stmt(p: *Parser): *ast.Stmt {
    if !consume(p, TOKEN_IMPORT) {
        return null;
    }

    let mod_path = lexer.empty_token();
    if !consume_token(p, TOKEN_STRING, &mod_path) {
        return null;
    }

    return ast.make_import_stmt(mod_path.span, mod_path.lexeme);
}

def parse_let_stmt(p: *Parser): *ast.Stmt {
    if !consume(p, TOKEN_LET) {
        return null;
    }

    let ident = lexer.empty_token();
    if !consume_token(p, TOKEN_IDENT, &ident) {
        return null;
    }

    if !consume(p, TOKEN_EQ) {
        return null;
    }

    let e = expression(p);
    if e == null {
        // TODO: create error
        return null;
    }
    return ast.make_let_stmt(ident, e);
}

def expression(p: *Parser): *ast.Expr {
    return parse_expression(p, precedence.assign());
}

def parse_expression(p: *Parser, precedence: i32): *ast.Expr {
    letcheck(left, prefix(p));

    while next_higher_precedence(p, precedence) {
        let token = lexer.empty_token();
        if !advance(p, &token) {
            delete left;
            return null;
        }

        let infix = infix(p, &token, left);
        if infix == null {
            delete left;
            return null;
        }

        left = infix;
    }

    return left;
}

def next_higher_precedence(p: *Parser, precedence: i32): bool {
    let t = peek(p);

    if t.ty == TOKEN_ERR || t.ty == TOKEN_EOF {
        return false;
    }

    return precedence.get(t.ty) > precedence;
}

def prefix(p: *Parser): *ast.Expr {
    let token = lexer.empty_token();
    if !advance(p, &token) {
        return null;
    }

    if token.ty == TOKEN_INT {
        let len = span.get(p.lexer.span_interner, token.span).len;
        return ast.make_int(token.span, len, token.lexeme);
    } else if token.ty == TOKEN_LPAREN {
        letcheck(e, expression(p));

        if !consume(p, TOKEN_RPAREN) {
            ast.expr_free(e);
            return null;
        }

        // TODO: this can over/underflow. Needs to be checked for interning
        e.span.start = e.span.start - 1;
        e.span.len_or_tag = e.span.len_or_tag + 2;

        return e;
    }

    return null;
}

def infix(p: *Parser, token: *lexer.Token, left: *ast.Expr): *ast.Expr {
    let found = false;
    let ty = 0;

    if token.ty == TOKEN_PLUS {
        found = true;
        ty = BINARY_ADD;
    } else if token.ty == TOKEN_MINUS {
        found = true;
        ty = BINARY_SUB;
    } else if token.ty == TOKEN_STAR {
        found = true;
        ty = BINARY_MUL;
    } else if token.ty == TOKEN_SLASH {
        found = true;
        ty = BINARY_DIV;
    }

    // a pattern matched
    if found {
        let prec = precedence.get(token.ty);
        letcheck(right, parse_expression(p, prec));
        let span = span.merge(p.lexer.span_interner, left.span, right.span);
        return ast.make_binary(span, ty, left, right);
    }

    return null;
}

def sync(p: *Parser): void {
    p.in_panic_mode = true;
    inner_sync(p);
    p.in_panic_mode = false;
}

// should only be called by sync function
def inner_sync(p: *Parser): void {
    let previous = lexer.empty_token();
    if !advance(p, &previous) {
        return;
    }

    let peek = peek(p);
    while peek.ty != TOKEN_EOF {
        if previous.ty == TOKEN_SEMI {
            return;
        }

        if peek.ty == TOKEN_LET {
            return;
        }

        if !advance(p, &previous) {
            return;
        }
        peek = peek(p);
    }
}

def create_error(span: span.Span, text: string): ParseError {
    return ParseError { span: span, text: text };
}

def create_statement_error(p: *Parser, start: i32, end: i32): ParseError {
    let span = lexer.create_span(&p.lexer, start, end);
    let len = span.get(p.lexer.span_interner, span).len;

    return create_error(span, std.err2str(ERROR_COULD_NOT_PARSE_STMT, len, p.lexer.source + start));
}

def create_lex_error(p: *Parser, next: *lexer.Token): ParseError {
    // len for lexeme
    let len = span.get(p.lexer.span_interner, next.span).len;

    return create_error(
        next.span,
        std.err2str(ERROR_UNKNOWN_SYMBOL, len, next.lexeme)
    );
}

def create_consume_error(p: *Parser, peek: *lexer.Token, expected_ty: i32): ParseError {
    let peek_str: string = null;
    let needs_free = lexer.token_to_string(peek, p.lexer.span_interner, &peek_str);

    let err_str = std.err2str(
        ERROR_EXPECTED_BUT_GOT,
        lexer.token_ty_to_static_string(expected_ty),
        peek_str
    );

    if needs_free {
        delete peek_str;
    }

    return create_error(peek.span, err_str);
}

