#include "./token_types.h"
#include "./expr_types.h"
#include "./error_code.h"
#include "./std.h"

#define CONSUME_RET_NULL(ty) if !consume(p, (ty)) { return null; } 0

import "lexer";
import "ast";
import "precedence";
import "span";
import "vec";
import "std";
import "mod";
import "param";
import "ty";

import "io"; // TODO: only debug

type ParseError struct {
    span: span.Span,
    text: string
}

type Parser struct {
    in_panic_mode: bool,
    lexer: lexer.Lexer,
    errors: vec.Vec // vector of ParseErrors
}

def create(source: string, span_interner: *span.SpanInterner, ctx: i32): Parser {
    return Parser {
        in_panic_mode: false,
        lexer: lexer.create(source, span_interner, ctx),
        errors: vec.create(sizeof ParseError)
    };
}

def free_p(p: *Parser): void {
    free_errs_from(p, 0);
    vec.free_v(&p.errors);
}

def free_errs_from(p: *Parser, start: i32): void {
    let i = start;
    let elem = ParseError { span: span.empty(), text: null };
    while i < p.errors.len {
        vec.get(&p.errors, i, &elem as *void);
        delete elem.text;
        i = i + 1;
    }
    p.errors.len = p.errors.len - (p.errors.len - start);
}

def num_errs(p: *Parser): i32 {
    return p.errors.len;
}

def get_err(p: *Parser, idx: i32, dest: *ParseError): bool {
    return vec.get(&p.errors, idx, dest as *void);
}

def push_err(p: *Parser, error: ParseError): void {
    vec.push(&p.errors, &error as *void);
}

// advances to the next token and saves the new token into dest
// dest: can be left empty if the result is to be ignored
// returns: true, if no error occured, false otherwise
def advance(p: *Parser, dest: *lexer.Token): bool {
    let next = lexer.next_token(&p.lexer);

    if next.ty == TOKEN_EOF {
        if !p.in_panic_mode {
            let error = create_error(next.span, std.err2str(ERROR_UNEXPECTED_EOF));
            vec.push(&p.errors, &error as *void);
        }
        return false;
    } else if next.ty == TOKEN_ERR && !p.in_panic_mode {
        // advance should not care about lex errors while in panic mode
        let error = create_lex_error(p, &next);
        push_err(p, error);
        return false;
    }

    if dest != null {
        *dest = next;
    }

    return true;
}

def consume(p: *Parser, ty: i32): bool {
    return consume_token(p, ty, null);
}

def consume_ident(p: *Parser, ident: *lexer.Token, allow_access_expr: bool): bool {
    if !consume_token(p, TOKEN_IDENT, ident) {
        return false;
    }

    if allow_access_expr {
        while peek(p).ty == TOKEN_DOT {
            consume(p, TOKEN_DOT);

            let first_part = *ident;
            if !consume_token(p, TOKEN_IDENT, ident) {
                return false;
            }

            let span = span.merge(p.lexer.span_interner, first_part.span, ident.span);
            *ident = lexer.create_token_from_span(
                &p.lexer,
                TOKEN_IDENT,
                first_part.lexeme,
                span
            );
        }
    }

    return true;
}

def consume_token(p: *Parser, ty: i32, dest: *lexer.Token): bool {
    let next = lexer.empty_token();
    if !advance(p, &next) {
        return false;
    }

    if dest != null {
        *dest = next;
    }

    if next.ty == ty {
        return true;
    }

    let error = create_consume_error(p, &next, ty);
    push_err(p, error);
    return false;
}

def consume_type(p: *Parser, dest: *ty.Type): bool {
    let num_errs = num_errs(p);

    let pointer_count = 0;
    while peek(p).ty == TOKEN_STAR {
        consume(p, TOKEN_STAR);
        pointer_count = pointer_count + 1;
    }

    let ty = lexer.empty_token();
    if !consume_ident(p, &ty, true) {
        if num_errs(p) > num_errs {
            free_errs_from(p, num_errs);
            let error = create_consume_error_text(p, &ty, "type");
            push_err(p, error);
        }

        return false;
    }
    *dest = ty.create_ptr(ident.create(ty), pointer_count);
    return true;
}

def peek(p: *Parser): lexer.Token {
    return lexer.peek(&p.lexer);
}

def parse(p: *Parser): mod.Module {
    let mod = mod.create();

    while peek(p).ty != TOKEN_EOF {
        let s = statement(p);
        if s != null {
            mod.push_stmt(&mod, s);
        }
    }

    return mod;
}

def statement(p: *Parser): *ast.Stmt {
    let num_errs = num_errs(p);
    let start = lexer.current_pos(&p.lexer);

    let s: *ast.Stmt = null;
    let consume_semi = parse_statement(p, &s);

    if s == null {
        let end = sync(p);

        // no parse error was added...
        if num_errs == num_errs(p) {
            // ... so just add a generic one
            let error = create_statement_error(p, start, end);
            push_err(p, error);
        }

        return null;
    }

    if consume_semi {
        if !consume(p, TOKEN_SEMI) {
            sync(p);
            ast.stmt_free(s);
            return null;
        }
    }

    return s;
}

def parse_statement(p: *Parser, dest: **ast.Stmt): bool {
    let peek = peek(p);
    if peek.ty == TOKEN_LET {
        *dest = parse_let_stmt(p);
        return true;
    } else if peek.ty == TOKEN_IMPORT {
        *dest = parse_import_stmt(p);
        return true;
    } else if peek.ty == TOKEN_DEF {
        *dest = parse_func_def(p);
        return false;
    }

    let expr = expression(p);
    if expr == null {
        *dest = null;
        return true;
    }

    *dest = ast.make_expr_stmt(expr);
    return true;
}

def parse_import_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_IMPORT);

    let mod_path = lexer.empty_token();
    if !consume_token(p, TOKEN_STRING, &mod_path) {
        return null;
    }

    return ast.make_import_stmt(mod_path.span, mod_path.lexeme);
}

def parse_let_stmt(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_LET);

    let ident = lexer.empty_token();
    if !consume_ident(p, &ident, false) {
        return null;
    }

    CONSUME_RET_NULL(TOKEN_EQ);

    let e = expression(p);
    if e == null {
        // TODO: create error
        return null;
    }
    return ast.make_let_stmt(ident, e);
}

def parse_func_def(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_DEF);

    let ident = lexer.empty_token();
    if !consume_ident(p, &ident, false) {
        return null;
    }

    CONSUME_RET_NULL(TOKEN_LPAREN);
    let params = parse_param_list(p, TOKEN_COMMA, TOKEN_RPAREN);
    let param_list = func.pl_from_vec(params);

    if !consume(p, TOKEN_COLON) {
        func.free_pl(&param_list);
        return null;
    }

    let ret_ty = ty.empty();
    if !consume_type(p, &ret_ty) {
        func.free_pl(&param_list);
        return null;
    }

    let block = parse_block(p);
    if block == null {
        func.free_pl(&param_list);
        return null;
    }

    return ast.make_func_decl_stmt(
        ident,
        param_list,
        ret_ty,
        ast.as_block_stmt(block)
    );
}

def free_statements(statements: *ptrvec.Vec): void {
    let i = 0;
    while i < statements.len {
        let s = ptrvec.get(statements, i) as *ast.Stmt;
        ast.stmt_free(s);
        i = i + 1;
    }

    ptrvec.free_v(statements);
}

def parse_block(p: *Parser): *ast.Stmt {
    CONSUME_RET_NULL(TOKEN_LBRACE);
    let statements = ptrvec.create();

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != TOKEN_RBRACE {
        let s = statement(p);
        if s == null {
            free_statements(&statements);
            return null;
        }

        ptrvec.push_ptr(&statements, s as *void);
        peek = peek(p);
    }

    if !consume(p, TOKEN_RBRACE) {
        free_statements(&statements);
        return null;
    }

    return ast.make_block_stmt(statements);
}

def parse_param_list(p: *Parser, sep_ty: i32, delim_ty: i32): vec.Vec {
#define FAIL() vec.free_v(&params); return vec.create(0)

    let params = vec.create(sizeof param.Param);

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != delim_ty {
        let ident = lexer.empty_token();
        if !consume_ident(p, &ident, false) {
            FAIL();
        }
        let ident = ident.create(ident);

        if !consume(p, TOKEN_COLON) {
            FAIL();
        }

        let ty = ty.empty();
        if !consume_type(p, &ty) {
            FAIL();
        }

        let param = param.create(ident, ty);
        vec.push(&params, &param as *void);

        peek = peek(p);

        // consume separator
        if peek.ty == sep_ty {
            consume(p, sep_ty);
        }
    }

    if !consume(p, delim_ty) {
        FAIL();
    }

    return params;

#undef FAIL
}

def parse_arg_list(p: *Parser, sep_ty: i32, delim_ty: i32): ast.ArgList {
#define FAIL() ast.free_al(&args); return ast.create_arg_list()
    let args = ast.create_arg_list();

    let peek = peek(p);
    while peek.ty != TOKEN_EOF && peek.ty != delim_ty {
        let arg = expression(p);
        if arg == null {
            FAIL();
        }
        ast.push_arg(&args, arg);

        peek = peek(p);

        if peek.ty == sep_ty {
            consume(p, sep_ty);
        }
    }

    return args;
#undef FAIL
}

def expression(p: *Parser): *ast.Expr {
    let left = parse_expression(p, precedence.assign());
    while peek(p).ty == TOKEN_EQ {
        consume(p, TOKEN_EQ);
        let value = expression(p);
        let span = span.merge(p.lexer.span_interner, left.span, value.span);
        left = ast.make_assign(span, left, value);
    }

    return left;
}

def parse_expression(p: *Parser, precedence: i32): *ast.Expr {
    letcheck(left, prefix(p));

    while next_higher_precedence(p, precedence) {
        let token = lexer.empty_token();
        if !advance(p, &token) {
            delete left;
            return null;
        }

        let infix = infix(p, &token, left);
        if infix == null {
            delete left;
            return null;
        }

        left = infix;
    }

    return left;
}

def next_higher_precedence(p: *Parser, precedence: i32): bool {
    let t = peek(p);

    if t.ty == TOKEN_ERR || t.ty == TOKEN_EOF {
        return false;
    }

    return precedence.get(t.ty) > precedence;
}

def prefix(p: *Parser): *ast.Expr {
    let token = lexer.empty_token();
    if !advance(p, &token) {
        return null;
    }

    if token.ty == TOKEN_INT {
        return ast.make_int(token.span, token.lexeme);
    } else if token.ty == TOKEN_STRING {
        return ast.make_string(token.span, token.lexeme);
    } else if token.ty == TOKEN_IDENT {
        if peek(p).ty == TOKEN_DOT {
            consume(p, TOKEN_DOT);

            let first_part = token;
            if !consume_ident(p, &token, true) {
                return null;
            }

            let span = span.merge(p.lexer.span_interner, first_part.span, token.span);
            token = lexer.create_token_from_span(
                &p.lexer,
                TOKEN_IDENT,
                first_part.lexeme,
                span
            );
        }

        return ast.make_ident(token);
    } else if token.ty == TOKEN_LPAREN {
        letcheck(e, expression(p));

        if !consume(p, TOKEN_RPAREN) {
            ast.expr_free(e);
            return null;
        }

        // TODO: this can over/underflow. Needs to be checked for interning
        e.span.start = e.span.start - 1;
        e.span.len_or_tag = e.span.len_or_tag + 2;

        return e;
    }

    return null;
}

def infix(p: *Parser, token: *lexer.Token, left: *ast.Expr): *ast.Expr {
    // call expr
    if token.ty == TOKEN_LPAREN {
        if ast.is_ident(left) {
            let ident = ast.as_ident(left);
            let args = parse_arg_list(p, TOKEN_COMMA, TOKEN_RPAREN);

            let closing_paren = lexer.empty_token();
            if !consume_token(p, TOKEN_RPAREN, &closing_paren) {
                ast.free_al(&args);
                return null;
            }

            let span = span.merge(p.lexer.span_interner, ident.e.span, closing_paren.span);
            return ast.make_call(span, ident, args);
        }
        return null;
    }

    // binary expression

    let found = false;
    let ty = 0;

    if token.ty == TOKEN_PLUS {
        found = true;
        ty = BINARY_ADD;
    } else if token.ty == TOKEN_MINUS {
        found = true;
        ty = BINARY_SUB;
    } else if token.ty == TOKEN_STAR {
        found = true;
        ty = BINARY_MUL;
    } else if token.ty == TOKEN_SLASH {
        found = true;
        ty = BINARY_DIV;
    }

    // a pattern matched
    if found {
        let prec = precedence.get(token.ty);
        letcheck(right, parse_expression(p, prec));
        let span = span.merge(p.lexer.span_interner, left.span, right.span);
        return ast.make_binary(span, ty, left, right);
    }

    return null;
}

def sync(p: *Parser): i32 {
    p.in_panic_mode = true;
    let pos = inner_sync(p);
    p.in_panic_mode = false;
    return pos;
}

// should only be called by sync function
def inner_sync(p: *Parser): i32 {
    let previous = lexer.empty_token();
    if !advance(p, &previous) {
        return lexer.end_pos(&p.lexer);
    }

    let peek = peek(p);
    while peek.ty != TOKEN_EOF {
        let span = span.get(p.lexer.span_interner, previous.span);
        let pos = span.start + span.len;
        if previous.ty == TOKEN_SEMI {
            return pos;
        }

        if peek.ty == TOKEN_LET || peek.ty == TOKEN_DEF {
            return pos;
        }

        if !advance(p, &previous) {
            return pos;
        }
        peek = peek(p);
    }

    let span = span.get(p.lexer.span_interner, previous.span);
    return span.start + span.len;
}

def create_error(span: span.Span, text: string): ParseError {
    return ParseError { span: span, text: text };
}

def create_statement_error(p: *Parser, start: i32, end: i32): ParseError {
    let span = lexer.create_span(&p.lexer, start, end);
    let len = span.get(p.lexer.span_interner, span).len;

    return create_error(span, std.err2str(ERROR_COULD_NOT_PARSE_STMT, len, p.lexer.source + start));
}

def create_lex_error(p: *Parser, next: *lexer.Token): ParseError {
    // len for lexeme
    let len = span.get(p.lexer.span_interner, next.span).len;

    return create_error(
        next.span,
        std.err2str(ERROR_UNKNOWN_SYMBOL, len, next.lexeme)
    );
}

def create_consume_error(p: *Parser, peek: *lexer.Token, expected_ty: i32): ParseError {
    return create_consume_error_text(
        p,
        peek,
        lexer.token_ty_to_static_string(expected_ty)
    );
}

def create_consume_error_text(p: *Parser, peek: *lexer.Token, text: string): ParseError {
    let peek_str: string = null;
    let needs_free = lexer.token_to_string(peek, p.lexer.span_interner, &peek_str);

    let err_str = std.err2str(
        ERROR_EXPECTED_BUT_GOT,
        text,
        peek_str
    );

    if needs_free {
        delete peek_str;
    }

    return create_error(peek.span, err_str);
}
